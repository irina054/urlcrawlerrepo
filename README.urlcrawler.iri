eBay : web application  crawl URL node point into given depth according to predefined pattern of network resources


1. Major Design Points:
	a. Validate input parameters: URL and depth
		1. URL format should be legal http request format
		2. depth should be number between 1 to 1000.  the limit could be changed.
		3. it something awkward about first level links and depth , it says that if will passed depth of 3, 
		   then on a first level will be generated 3 links -1, -2, -3, but sub-links were generated only for first 2 (-1, -2) and not for (-3). why???
		   So, in implemented software , there is an option to separate : 1st level width and depth. But currently, first level width is initialized with depth number, so, 
		   if depth is 3, then first level is 3 elements, means totally will be checked 21 URLs , and not 14 as defined in a spec... looks like some mistake in a spec....
		   Looks like, if 1-st level width is 2 and depth is 3 then totally should be checked 14 links.
		
	b. Parse URL to separate prefix and suffix to produce all related URLs. For example , if input URL is : 
		http://www.example.com/docs/resource.com  
		
		then all produced URLs will be as : 
		
		http://www.example.com/docs/resource-1.com
		http://www.example.com/docs/resource-2.com
		http://www.example.com/docs/resource-3.com

		http://www.example.com/docs/resource-1-1.com
		http://www.example.com/docs/resource-1-2.com

		http://www.example.com/docs/resource-2-1.com
		http://www.example.com/docs/resource-2-2.com

		http://www.example.com/docs/resource-3-1.com
		http://www.example.com/docs/resource-3-2.com

		Current implementation doesn't perform parsing , it assumes to get prefix of URL, and it concatenate default suffix ".com"... could be added easily.
		So, to get above output the input should be "http://www.example.com/docs/resource"
		
	c. Generate all required URLs based on input parameters : URL and depth
		1. prepared some basic infra to customize delimiters of generated resources 
		2. 
	
	d. Run URL checker for each generated resources.
	
	d. Some basic junit test were prepared to validate number of generated links... not content...
		Other tests shoud be prepared as well: for each component 
	
	e. Used HttpUrlConnection to test URL, all other protocols are not supported : file, ftp, https, etc... could be easly added
	
	f. Modular Design
	=========================================================================================
		1. All basic components were defined with appropriate interfaces to be able extend current implementation and replace parts:
		       PatternGenerator
			   ExecutorService
			   Factory of checker execution by protocol
	g. Provided basic infra to run in parallel few checkers , limited by number of threads
	=========================================================================================

        h. I've uploaded few appropriate files into resources/static folder in a required format to check positive behaviour - file exist,
	=========================================================================================
	   To use it run REST API as follows: 
		curl 'desktop-iri:8080//crawle?url=http://desktop-iri:8080/node&depth=3'
		
		this will use same instance of spring tomcat to check our resources  (my computer hostname is : desktop-iri)
		the resources with HTTP code 200 are : node-1.com  and node-2-1.com 
		



2. Run on your computer urlcrawler-spring-bootable application by one of two ways :
=========================================================================================


   java -jar  target/urlcrawler-spring-bootable-0.0.1-SNAPSHOT.jar

or

   mvn spring-boot:run


     j. Check web application access :
	=========================================================================================

run command :  hostname
DESKTOP-iri

http://desktop-iri:8080/
(all URLs will work the same : "/",  "/hello", "/hello/")

the result is :

Hello from IriSpringApp :)


3. Sample of activation on my computer: 
=========================================================================================
$ curl 'desktop-iri:8080//crawle?url=http://desktop-iri:8080/node&depth=3'

##########################################################
#case when few resources are found , see below status 200.
[{"url":"http://desktop-iri:8080/node-1.com","httpRequestStatusCode":200},{"url":"http://desktop-iri:8080/node-2.com","httpRequestStatusCode":404},{"url":"http://deskto
p-iri:8080/node-3.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-1-1.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-1-2.
com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-2-1.com","httpRequestStatusCode":200},{"url":"http://desktop-iri:8080/node-2-2.com","httpRequestS
tatusCode":404},{"url":"http://desktop-iri:8080/node-3-1.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-3-2.com","httpRequestStatusCode":404},{"
url":"http://desktop-iri:8080/node-1-1-1.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-1-1-2.com","httpRequestStatusCode":404},{"url":"http://d
esktop-iri:8080/node-1-2-1.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-1-2-2.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:808
0/node-2-1-1.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-2-1-2.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-2-2-1.c
om","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-2-2-2.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-3-1-1.com","httpReque
stStatusCode":404},{"url":"http://desktop-iri:8080/node-3-1-2.com","httpRequestStatusCode":404},{"url":"http://desktop-iri:8080/node-3-2-1.com","httpRequestStatusCode":
404},{"url":"http://desktop-iri:8080/node-3-2-2.com","httpRequestStatusCode":404}]
itransky@DESKTOP-iri /cygdrive/c/Irina/IriJavaProjectsPractice/urlcrawler
$



#######################################################
#its a case of existed host and resources are not exist
$ curl 'desktop-iri:8080//crawle?url=http://www.google.com/from-node&depth=2'
[{"url":"http://www.google.com/from-node-1.com","httpRequestStatusCode":404},{"url":"http://www.google.com/from-node-2.com","httpRequestStatusCode":404},{"url":"http://
www.google.com/from-node-1-1.com","httpRequestStatusCode":404},{"url":"http://www.google.com/from-node-1-2.com","httpRequestStatusCode":404},{"url":"http://www.google.c
om/from-node-2-1.com","httpRequestStatusCode":404},{"url":"http://www.google.com/from-node-2-2.com","httpRequestStatusCode":404}]
itransky@DESKTOP-iri /cygdrive/c/Irina/IriJavaProjectsPractice/urlcrawler


################################################
#its a case of wrong URL: host doesn't exist
$ curl 'desktop-iri:8080//crawle?url=http://from-node&depth=2'
[{"url":"http://from-node-1.com","httpRequestStatusCode":400},{"url":"http://from-node-2.com","httpRequestStatusCode":400},{"url":"http://from-node-1-1.com","httpReques
tStatusCode":400},{"url":"http://from-node-1-2.com","httpRequestStatusCode":400},{"url":"http://from-node-2-1.com","httpRequestStatusCode":400},{"url":"http://from-node
-2-2.com","httpRequestStatusCode":400}]
itransky@DESKTOP-iri /cygdrive/c/Irina/IriJavaProjectsPractice/urlcrawler
$

